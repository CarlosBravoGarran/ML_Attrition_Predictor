{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carlos Bravo Garrán - 100474964"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # __Exploratory Data Analysis (EDA)__ \n",
    "\n",
    " In this notebook, a simplified Exploratory Data Analysis (EDA) will be performed on the provided dataset, with the objective of analyzing and understanding the factors that influence employee attrition in an organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. __Load Libraries and Data__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis and wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, recall_score, roc_curve, auc, confusion_matrix, classification_report)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import randint\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "# Warnings configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create virtual environment and install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install venv and requirements\n",
    "# !python3 -m venv venv\n",
    "# !source venv/bin/activate\n",
    "# %pip install -r ../requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from the CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/attrition_availabledata_03.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. __Initial Exploration__\n",
    "\n",
    "Review the general structure of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shape = df.shape\n",
    "print(f\"The dataset contains {dataset_shape[0]} rows and {dataset_shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Attrition']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a __classification__ problem, as the target variable (Attrition) is binary (Yes / No). This means that the model must predict whether an employee will leave the company or not, rather than predicting a numerical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. __Identify Categorical and Numerical Variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "print(\"Categorical variables:\", categorical_columns)\n",
    "print(\"Numerical variables:\", numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Reclassify variables by adding ordinals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "ordinal_columns = [\"Education\", \"JobLevel\", \"EnvironmentSatisfaction\", \"JobSatisfaction\", \"WorkLifeBalance\", \"PerformanceRating\", \"StockOptionLevel\"]\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Remove from numerical the ones we have classified as ordinal\n",
    "numerical_columns = [col for col in numerical_columns if col not in ordinal_columns]\n",
    "\n",
    "print(\"Categorical variables:\", categorical_columns)\n",
    "print(\"Ordinal variables:\", ordinal_columns)\n",
    "print(\"Numerical variables:\", numerical_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Detect high cardinality categorical variables\n",
    "\n",
    "Identify categorical variables that may generate too many columns when encoding them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cardinality = df[categorical_columns].nunique().sort_values(ascending=False)\n",
    "display(categorical_cardinality)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not considered that there are high cardinality categorical variables, therefore no additional grouping or different encoding will be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. __Analysis of the Target Variable__\n",
    "\n",
    "Review the distribution of the target variable to identify class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Attrition\" in df.columns:\n",
    "    plt.figure(figsize=(4,4))\n",
    "    sns.countplot(x=df[\"Attrition\"], palette=\"viridis\")\n",
    "    plt.title(\"Distribution of the Target Variable (Attrition)\\n\")\n",
    "    plt.show()\n",
    "    \n",
    "    attrition_counts = df[\"Attrition\"].value_counts(normalize=True)\n",
    "    display(pd.DataFrame(attrition_counts).rename(columns={\"Attrition\": \"Proportion\"}).reset_index(drop=True)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Attrition.value_counts().sort_index().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is imbalanced, with 2466 employees not leaving the company (NO) and 474 who do leave (YES).\n",
    "\n",
    "This means that the majority of employees do not leave the company, which could cause a poorly trained model to always predict \"No\", achieving an apparently high accuracy, but without actually capturing the cases of attrition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. __Identify Missing Values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0].to_frame().reset_index()\n",
    "missing_values.columns = [\"Column Name\", \"Missing Values\"]\n",
    "\n",
    "display(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the missing values are very few compared to the total number of data, we will take the following measures:\n",
    "- Impute missing values in numerical variables with the median\n",
    "- Impute missing values in categorical variables with the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns differentiated by type\n",
    "num_cols = [col for col in df.select_dtypes(include=['number']).columns if col not in ordinal_columns]\n",
    "ord_cols = ordinal_columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. __Identify Constant and Identifying Variables__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Check for columns with unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df.nunique()\n",
    "\n",
    "constant_columns = df.nunique()[df.nunique() == 1].to_frame().reset_index()\n",
    "constant_columns.columns = [\"Column Name\", \"Unique Value Count\"]\n",
    "constant_columns[\"Unique Value\"] = constant_columns[\"Column Name\"].apply(lambda col: df[col].unique()[0])\n",
    "constant_columns_list = constant_columns[\"Column Name\"].tolist()\n",
    "\n",
    "display(constant_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Check for columns with ID variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of rows: {len(df)}\")\n",
    "\n",
    "unique_values = df.nunique()\n",
    "print(\"Number of unique values per column:\")\n",
    "print(unique_values, \"\\n\")\n",
    "\n",
    "id_columns = [col for col in df.columns if df[col].nunique() == len(df)]\n",
    "print(\"Identifying columns detected:\", id_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As its name indicates, _EmployeeID_ is an identifying variable, making it redundant for the study.\n",
    "\n",
    "On the other hand, the _hours_ column has 2939 values, 1 less than the total number of rows. It might be considered identifying, but upon checking the values, we see that they are simply different decimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Remove Constant and Identifying Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=constant_columns_list + id_columns, errors='ignore')\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "print('The following columns have been removed:', constant_columns_list + id_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. __Create Correlation Matrix__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the correlation matrix to understand relationships between numerical variables, having already removed constant and identifying variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df.select_dtypes(include=['number'])\n",
    "\n",
    "correlation_matrix = df_numeric.corr()\n",
    "\n",
    "plt.figure(figsize=(18, 18))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", linewidths=0.5, cmap=\"coolwarm\")\n",
    "plt.xticks(rotation=75)\n",
    "plt.title(\"Correlation Matrix\\n\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no extremely high correlations, close to 1 or -1, so the variables are not redundant nor strongly dependent on each other. However, there are some moderate correlations that we can consider.\n",
    "\n",
    "__Relationship between tenure and work experience:__\n",
    "\n",
    "- _YearsAtCompany_ and _YearsWithCurrManager_ (0.76): Employees who have been with the company longer are more likely to have been with the same manager for a longer time.\n",
    "- _YearsAtCompany_ and _TotalWorkingYears_ (0.62): The longer a person has worked in general, the more time they may have spent at the current company.\n",
    "\n",
    "__Relationship between _PercentSalaryHike_ and _PerformanceRating_ (0.78):__ There is a high correlation between salary increase and performance rating. Employees with better performance receive higher salary increases.\n",
    "\n",
    "- It could be evaluated if _PercentSalaryHike_ is redundant, as it is strongly linked to _PerformanceRating_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the rest, they have very low correlations with all the others, indicating that they may be independent or influenced by other factors not considered. Thus, it could be reviewed if these variables have any significant impact on the target variable, or if they can be eliminated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. __Identify Correlation with the Target Variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target variable to numeric\n",
    "df_aux = df.copy()\n",
    "df_aux[\"Attrition\"] = df_aux[\"Attrition\"].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "# Ensure we only work with numeric columns\n",
    "df_corr = df_aux.select_dtypes(include=['number'])\n",
    "attrition_correlation = df_corr.corr()[\"Attrition\"].sort_values()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(y=attrition_correlation.index, x=attrition_correlation.values, palette=\"coolwarm\")\n",
    "\n",
    "# Add values on the bars\n",
    "for index, value in enumerate(attrition_correlation.values):\n",
    "    ax.text(value, index, f\"{value:.2f}\", ha=\"left\", va=\"center\", fontsize=10, color=\"black\")\n",
    "\n",
    "plt.title(\"Correlation of 'Attrition' with other variables\")\n",
    "plt.xlabel(\"Correlation\")\n",
    "plt.ylabel(\"Variables\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede comprobar que no hay una variable con una correlación extremadamente fuerte con Attrition, pero sí que muchas de ellas tienen una relación muy baja, por ello cabría la posibilidad de considerar su elimiación con el fin de simplificar el modelo.\n",
    "\n",
    "Junto con las horas trabajadas, las variables de antigüedad (_YearsAtCompany_, _TotalWorkingYears_) son las que más influyen en la retención, se pueden evaluar las relaciones con _YearsWithCurrManager_ para comprobar si esta última sería redundante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se podrían eliminar las variables que tienen muy baja correlación con la objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter variables with correlation less than 0.05 in absolute value\n",
    "low_corr_columns = attrition_correlation[abs(attrition_correlation) < 0.05].index.tolist()\n",
    "\n",
    "# Remove these columns from the dataset\n",
    "df_filtered = df.drop(columns=low_corr_columns, errors='ignore')\n",
    "\n",
    "print(f\"The following columns have been removed in a test dataframe due to low correlation: {low_corr_columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __9. Visualize Relationships Between Correlated Variables__\n",
    "Perform a visual exploration of the relationships between highly correlated variables to help decide if there are redundancies or if some variables need to be transformed before using them in a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationships between tenure variables with pairplot\n",
    "sns.pairplot(df, vars=[\"YearsAtCompany\", \"YearsWithCurrManager\", \"TotalWorkingYears\"], diag_kind=\"kde\")\n",
    "plt.suptitle(\"Relationships between Tenure and Work Experience\", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relationship between Tenure and Work Experience__\n",
    "\n",
    "- _YearsAtCompany_ and _YearsWithCurrManager_ (0.76): Strong relationship, possible redundancy.\n",
    "- _YearsAtCompany_ and _TotalWorkingYears_ (0.62): Expected relationship, but provides different information.\n",
    "- Conclusion: _YearsAtCompany_ or _YearsWithCurrManager_ could be redundant.\n",
    "\n",
    "It could be evaluated for impact on the model and one could be removed if necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having identified and evaluated the variables, checking for null values, constant and identifying values, and correlations between them, we can proceed to the evaluation of potential classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Evaluation of Classification Models with Advanced Preprocessing__\n",
    "In this section, a preprocessing and modeling pipeline is implemented to predict the Attrition variable in an employee dataset. Cross-validation will be used for internal evaluation (inner evaluation) and a final evaluation with an independent test set (outer evaluation).\n",
    "\n",
    "The main steps followed are:\n",
    "\n",
    "1. Splitting data into training and test sets (2/3 for training the model - 1/3 for evaluating final performance).\n",
    "2. Data preprocessing, including imputation, scaling, encoding, and dimensionality reduction.\n",
    "3. Internal evaluation (inner evaluation) using stratified cross-validation.\n",
    "4. Training and final evaluation (outer evaluation) with key metrics such as balanced accuracy, accuracy, TPR, TNR, and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1. Data Splitting into Train and Test__\n",
    "\n",
    "The predictor variables (X) are separated from the target variable (y) and the data is split into training and test sets.Se separan las variables predictoras (X) de la variable objetivo (y) y realiza la división de los datos en conjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Attrition\"])\n",
    "y = df[\"Attrition\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=1/3, random_state=100474964)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2. Data Preprocessing__\n",
    "The preprocessing is divided into three types of variables:\n",
    "\n",
    "- Numerical variables: Imputed with KNNImputer and scaled with RobustScaler.\n",
    "- Categorical variables: Imputed with the mode (most_frequent) and encoded with OneHotEncoder, followed by dimensionality reduction with PCA(n_components=5).\n",
    "- Ordinal variables: Imputed with the median and transformed with OrdinalEncoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Identification of Variable Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = X.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "ordinal_columns = [\"Education\", \"JobLevel\", \"EnvironmentSatisfaction\", \"JobSatisfaction\", \"WorkLifeBalance\", \"PerformanceRating\", \"StockOptionLevel\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Transformations by Variable Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Label Encoding to ordinal variables\n",
    "ord_encoder = OrdinalEncoder()\n",
    "df[ordinal_columns] = ord_encoder.fit_transform(df[ordinal_columns])\n",
    "\n",
    "ord_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "# Pipeline for categorical data\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "    ('pca', PCA(n_components=5))\n",
    "])\n",
    "\n",
    "# Pipeline for numerical data\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# ColumnTransformer with all preprocessing steps\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_transformer, numerical_columns),\n",
    "    ('cat', cat_transformer, categorical_columns),\n",
    "    ('ord', ord_transformer, ordinal_columns)\n",
    "])\n",
    "\n",
    "preprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __3. Conversion of Target Variable (Attrition)__\n",
    "Convert the Attrition variable to a binary format (1 for \"Yes\" and 0 for \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.map({\"Yes\": 1, \"No\": 0})\n",
    "y_test = y_test.map({\"Yes\": 1, \"No\": 0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __4. Application of the Preprocessor__\n",
    "The preprocessor is fitted and transforms the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Evaluates a classification model and displays key metrics, confusion matrix, and ROC curve.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    tpr = recall_score(y_test, y_pred)  # Sensitivity / TPR\n",
    "    tnr = recall_score(y_test, y_pred, pos_label=0)  # Specificity / TNR\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"=== Final Evaluation: {model_name} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {bal_acc:.4f}\")\n",
    "    print(f\"TPR (Sensitivity): {tpr:.4f}\")\n",
    "    print(f\"TNR (Specificity): {tnr:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(f\"Confusion Matrix ({model_name})\")\n",
    "    plt.show()\n",
    "\n",
    "    if y_prob is not None:\n",
    "        fpr, tpr_curve, _ = roc_curve(y_test, y_prob)\n",
    "        auc_score = auc(fpr, tpr_curve)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr_curve, label=f'{model_name} (AUC = {auc_score:.4f})', color='blue')\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  \n",
    "        \n",
    "        plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "        plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "        plt.title(f\"ROC Curve for {model_name}\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "        return {\"Accuracy\": acc, \"Balanced Accuracy\": bal_acc, \"TPR\": tpr, \"TNR\": tnr, \"AUC\": auc_score}\n",
    "    \n",
    "    return {\"Accuracy\": acc, \"Balanced Accuracy\": bal_acc, \"TPR\": tpr, \"TNR\": tnr, \"AUC\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __5. Evaluation with Dummy Model__\n",
    "\n",
    "A Dummy model is trained to establish reference points:\n",
    "\n",
    "- Expected Balanced Accuracy if the model were trivial.\n",
    "- Comparison with trained models to demonstrate their effectiveness.\n",
    "\n",
    "This will verify if the models are truly learning patterns or simply reflecting class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train_transformed, y_train)\n",
    "dummy_ev = evaluate_model(dummy, X_test_transformed, y_test, \"Dummy Model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __6. Definition of Models with Pipeline__\n",
    "A pipeline is built that includes preprocessing and the classification model (_DecisionTreeClassifier_ and _KNeighborsClassifier_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', tree.DecisionTreeClassifier(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "clf_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knn = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "clf_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __7. Internal Evaluation (Inner Evaluation)__\n",
    "For internal evaluation, stratified cross-validation (StratifiedKFold) with 5 splits (n_splits=5) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=100474964)\n",
    "cross_val_scores = cross_val_score(clf_tree, X_train, y_train, cv=skf, scoring='balanced_accuracy')\n",
    "print(f\"Balanced Accuracy (inner evaluation): {np.mean(cross_val_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __8. Modelo Train__\n",
    "The model is fitted with the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree.fit(X_train, y_train)\n",
    "clf_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __9. Evaluation with Test (Outer Evaluation)__\n",
    "Predictions are made on the test set and the main metrics are calculated:\n",
    "\n",
    "- Balanced Accuracy: Average of TPR and TNR.\n",
    "- Accuracy: Proportion of correct predictions.\n",
    "- TPR (Sensitivity/Recall): How well the model detects positive cases.\n",
    "- TNR (Specificity): How well the model detects negative cases.\n",
    "- Confusion Matrix: Detailed visualization of hits and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tree_ev = evaluate_model(clf_tree, X_test, y_test, \"Decision Tree (Base)\")\n",
    "knn_ev = evaluate_model(clf_knn, X_test, y_test, \"KNN (Base)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __10. Model Optimization with RandomizedSearchCV__\n",
    "\n",
    "Randomized hyperparameter search (_RandomizedSearchCV_) is used to find the best configuration for the models to improve their performance by efficiently tuning their hyperparameters.\n",
    "\n",
    "- KNN (_n_neighbors_, _weights_, _metric_).  \n",
    "- Decision Tree (_max_depth_, _criterion_).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist_knn = {\"n_neighbors\": randint(3, 20), \"weights\": [\"uniform\", \"distance\"], \"metric\": [\"euclidean\", \"manhattan\"]}\n",
    "param_dist_tree = {\"max_depth\": randint(3, 20), \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "random_knn = RandomizedSearchCV(KNeighborsClassifier(), \n",
    "                                param_distributions=param_dist_knn, \n",
    "                                n_iter=10, cv=5, \n",
    "                                scoring=\"balanced_accuracy\", \n",
    "                                n_jobs=-1, random_state=100474964)\n",
    "random_tree = RandomizedSearchCV(DecisionTreeClassifier(), \n",
    "                                 param_distributions=param_dist_tree, \n",
    "                                 n_iter=10, cv=5, \n",
    "                                 scoring=\"balanced_accuracy\", \n",
    "                                 n_jobs=-1, random_state=100474964)\n",
    "\n",
    "random_knn.fit(X_train_transformed, y_train)\n",
    "random_tree.fit(X_train_transformed, y_train)\n",
    "\n",
    "print(f\"Best hyperparameters for KNN: {random_knn.best_params_}\")\n",
    "print(f\"Balanced Accuracy KNN: {random_knn.best_score_:.4f}\")\n",
    "print(f\"Best hyperparameters for Decision Tree: {random_tree.best_params_}\")\n",
    "print(f\"Balanced Accuracy Decision Tree: {random_tree.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __11. Evaluation of Optimized Models__\n",
    "\n",
    "To confirm that hyperparameter optimization actually improves performance:\n",
    "- The best hyperparameters are selected and the final models are trained.\n",
    "- All metrics are recalculated to see the improvement over the default models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_tree_ev = evaluate_model(random_tree.best_estimator_, X_test_transformed, y_test, \"Decision Tree (Optimized)\")\n",
    "opt_knn_ev = evaluate_model(random_knn.best_estimator_, X_test_transformed, y_test, \"KNN (Optimized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Use of AI in the Development of the Work__\n",
    "In the development of this work, artificial intelligence (ChatGPT) has been used as a support tool to facilitate the understanding of concepts, receive methodological and code suggestions, and improve the documentation writing.\n",
    "\n",
    "#### __1. Generation of Ideas and Recommendations__\n",
    "\n",
    "For project planning and model implementation, AI has been used to obtain methodological suggestions and alternative approaches in the following aspects:\n",
    "\n",
    "- Selection of preprocessing methods: Different strategies for data imputation, variable scaling, and categorical data encoding have been explored.\n",
    "- Model comparison: Recommendations have been received on how to structure the evaluation of base models versus optimized models through hyperparameter tuning.\n",
    "- Code improvement: It has helped improve and refactor some code blocks to enhance their efficiency and readability.\n",
    "\n",
    "#### __2. Explanation of Concepts__\n",
    "\n",
    "At certain points during the development, AI has been consulted for detailed explanations on technical aspects such as:\n",
    "\n",
    "- Meaning and interpretation of metrics like balanced accuracy, TPR, and TNR.\n",
    "- Differences between internal evaluation (inner evaluation) and external evaluation (outer evaluation).\n",
    "- Hyperparameter optimization using RandomizedSearchCV and its application in model selection.\n",
    "\n",
    "#### __3. Support in Documentation Writing__\n",
    "For the preparation of the report, AI has served as an assistant in:\n",
    "\n",
    "- Organizing sections and document structure.\n",
    "- Writing clearer and more concise explanations.\n",
    "- Formulating conclusions based on the obtained results.\n",
    "- Style correction and clarity in the exposition of ideas.\n",
    "- Writing the section 'Use of AI in the Development of the Work'\n",
    "\n",
    "The use of AI in this work has been strictly as support in the learning process, idea generation, and documentation writing, always supervising and verifying the results without replacing the execution of analyses, code implementation, or result interpretation. All final decisions have been made autonomously, based on the analysis and understanding of the problem.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
