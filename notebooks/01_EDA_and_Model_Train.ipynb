{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carlos Bravo Garrán - 100474964"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # __Exploratory Data Analysis (EDA)__ \n",
    "\n",
    " In this notebook, a simplified Exploratory Data Analysis (EDA) will be performed on the provided dataset, with the objective of analyzing and understanding the factors that influence employee attrition in an organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. __Load Libraries and Data__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis and wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, recall_score, roc_curve, auc, confusion_matrix, classification_report)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import randint\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "# Warnings configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create virtual environment and install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install venv and requirements\n",
    "# !python3 -m venv venv\n",
    "# !source venv/bin/activate\n",
    "# %pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from the CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/attrition_availabledata_03.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. __Initial Exploration__\n",
    "\n",
    "Review the general structure of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shape = df.shape\n",
    "print(f\"The dataset contains {dataset_shape[0]} rows and {dataset_shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Attrition']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a __classification__ problem, as the target variable (Attrition) is binary (Yes / No). This means that the model must predict whether an employee will leave the company or not, rather than predicting a numerical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. __Identify Categorical and Numerical Variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "print(\"Categorical variables:\", categorical_columns)\n",
    "print(\"Numerical variables:\", numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Reclassify variables by adding ordinals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "ordinal_columns = [\"Education\", \"JobLevel\", \"EnvironmentSatisfaction\", \"JobSatisfaction\", \"WorkLifeBalance\", \"PerformanceRating\", \"StockOptionLevel\"]\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Remove from numerical the ones we have classified as ordinal\n",
    "numerical_columns = [col for col in numerical_columns if col not in ordinal_columns]\n",
    "\n",
    "print(\"Categorical variables:\", categorical_columns)\n",
    "print(\"Ordinal variables:\", ordinal_columns)\n",
    "print(\"Numerical variables:\", numerical_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Detect high cardinality categorical variables\n",
    "\n",
    "Identify categorical variables that may generate too many columns when encoding them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cardinality = df[categorical_columns].nunique().sort_values(ascending=False)\n",
    "display(categorical_cardinality)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not considered that there are high cardinality categorical variables, therefore no additional grouping or different encoding will be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. __Analysis of the Target Variable__\n",
    "\n",
    "Review the distribution of the target variable to identify class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Attrition\" in df.columns:\n",
    "    plt.figure(figsize=(4,4))\n",
    "    sns.countplot(x=df[\"Attrition\"], palette=\"viridis\")\n",
    "    plt.title(\"Distribution of the Target Variable (Attrition)\\n\")\n",
    "    plt.show()\n",
    "    \n",
    "    attrition_counts = df[\"Attrition\"].value_counts(normalize=True)\n",
    "    display(pd.DataFrame(attrition_counts).rename(columns={\"Attrition\": \"Proportion\"}).reset_index(drop=True)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Attrition.value_counts().sort_index().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is imbalanced, with 2466 employees not leaving the company (NO) and 474 who do leave (YES).\n",
    "\n",
    "This means that the majority of employees do not leave the company, which could cause a poorly trained model to always predict \"No\", achieving an apparently high accuracy, but without actually capturing the cases of attrition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. __Identify Missing Values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0].to_frame().reset_index()\n",
    "missing_values.columns = [\"Column Name\", \"Missing Values\"]\n",
    "\n",
    "display(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the missing values are very few compared to the total number of data, we will take the following measures:\n",
    "- Impute missing values in numerical variables with the median\n",
    "- Impute missing values in categorical variables with the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns differentiated by type\n",
    "num_cols = [col for col in df.select_dtypes(include=['number']).columns if col not in ordinal_columns]\n",
    "ord_cols = ordinal_columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. __Identify Constant and Identifying Variables__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Check for columns with unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df.nunique()\n",
    "\n",
    "constant_columns = df.nunique()[df.nunique() == 1].to_frame().reset_index()\n",
    "constant_columns.columns = [\"Column Name\", \"Unique Value Count\"]\n",
    "constant_columns[\"Unique Value\"] = constant_columns[\"Column Name\"].apply(lambda col: df[col].unique()[0])\n",
    "constant_columns_list = constant_columns[\"Column Name\"].tolist()\n",
    "\n",
    "display(constant_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Check for columns with ID variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of rows: {len(df)}\")\n",
    "\n",
    "unique_values = df.nunique()\n",
    "print(\"Number of unique values per column:\")\n",
    "print(unique_values, \"\\n\")\n",
    "\n",
    "id_columns = [col for col in df.columns if df[col].nunique() == len(df)]\n",
    "print(\"Identifying columns detected:\", id_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As its name indicates, _EmployeeID_ is an identifying variable, making it redundant for the study.\n",
    "\n",
    "On the other hand, the _hours_ column has 2939 values, 1 less than the total number of rows. It might be considered identifying, but upon checking the values, we see that they are simply different decimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Remove Constant and Identifying Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=constant_columns_list + id_columns, errors='ignore')\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "print('The following columns have been removed:', constant_columns_list + id_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. __Create Correlation Matrix__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the correlation matrix to understand relationships between numerical variables, having already removed constant and identifying variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df.select_dtypes(include=['number'])\n",
    "\n",
    "correlation_matrix = df_numeric.corr()\n",
    "\n",
    "plt.figure(figsize=(18, 18))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", linewidths=0.5, cmap=\"coolwarm\")\n",
    "plt.xticks(rotation=75)\n",
    "plt.title(\"Correlation Matrix\\n\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no extremely high correlations, close to 1 or -1, so the variables are not redundant nor strongly dependent on each other. However, there are some moderate correlations that we can consider.\n",
    "\n",
    "__Relationship between tenure and work experience:__\n",
    "\n",
    "- _YearsAtCompany_ and _YearsWithCurrManager_ (0.76): Employees who have been with the company longer are more likely to have been with the same manager for a longer time.\n",
    "- _YearsAtCompany_ and _TotalWorkingYears_ (0.62): The longer a person has worked in general, the more time they may have spent at the current company.\n",
    "\n",
    "__Relationship between _PercentSalaryHike_ and _PerformanceRating_ (0.78):__ There is a high correlation between salary increase and performance rating. Employees with better performance receive higher salary increases.\n",
    "\n",
    "- It could be evaluated if _PercentSalaryHike_ is redundant, as it is strongly linked to _PerformanceRating_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the rest, they have very low correlations with all the others, indicating that they may be independent or influenced by other factors not considered. Thus, it could be reviewed if these variables have any significant impact on the target variable, or if they can be eliminated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. __Identify Correlation with the Target Variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target variable to numeric\n",
    "df_aux = df.copy()\n",
    "df_aux[\"Attrition\"] = df_aux[\"Attrition\"].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "# Ensure we only work with numeric columns\n",
    "df_corr = df_aux.select_dtypes(include=['number'])\n",
    "attrition_correlation = df_corr.corr()[\"Attrition\"].sort_values()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(y=attrition_correlation.index, x=attrition_correlation.values, palette=\"coolwarm\")\n",
    "\n",
    "# Add values on the bars\n",
    "for index, value in enumerate(attrition_correlation.values):\n",
    "    ax.text(value, index, f\"{value:.2f}\", ha=\"left\", va=\"center\", fontsize=10, color=\"black\")\n",
    "\n",
    "plt.title(\"Correlation of 'Attrition' with other variables\")\n",
    "plt.xlabel(\"Correlation\")\n",
    "plt.ylabel(\"Variables\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede comprobar que no hay una variable con una correlación extremadamente fuerte con Attrition, pero sí que muchas de ellas tienen una relación muy baja, por ello cabría la posibilidad de considerar su elimiación con el fin de simplificar el modelo.\n",
    "\n",
    "Junto con las horas trabajadas, las variables de antigüedad (_YearsAtCompany_, _TotalWorkingYears_) son las que más influyen en la retención, se pueden evaluar las relaciones con _YearsWithCurrManager_ para comprobar si esta última sería redundante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se podrían eliminar las variables que tienen muy baja correlación con la objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter variables with correlation less than 0.05 in absolute value\n",
    "low_corr_columns = attrition_correlation[abs(attrition_correlation) < 0.05].index.tolist()\n",
    "\n",
    "# Remove these columns from the dataset\n",
    "df_filtered = df.drop(columns=low_corr_columns, errors='ignore')\n",
    "\n",
    "print(f\"The following columns have been removed in a test dataframe due to low correlation: {low_corr_columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __9. Visualize Relationships Between Correlated Variables__\n",
    "Perform a visual exploration of the relationships between highly correlated variables to help decide if there are redundancies or if some variables need to be transformed before using them in a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationships between tenure variables with pairplot\n",
    "sns.pairplot(df, vars=[\"YearsAtCompany\", \"YearsWithCurrManager\", \"TotalWorkingYears\"], diag_kind=\"kde\")\n",
    "plt.suptitle(\"Relationships between Tenure and Work Experience\", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relationship between Tenure and Work Experience__\n",
    "\n",
    "- _YearsAtCompany_ and _YearsWithCurrManager_ (0.76): Strong relationship, possible redundancy.\n",
    "- _YearsAtCompany_ and _TotalWorkingYears_ (0.62): Expected relationship, but provides different information.\n",
    "- Conclusion: _YearsAtCompany_ or _YearsWithCurrManager_ could be redundant.\n",
    "\n",
    "It could be evaluated for impact on the model and one could be removed if necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having identified and evaluated the variables, checking for null values, constant and identifying values, and correlations between them, we can proceed to the evaluation of potential classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Evaluation of Classification Models with Advanced Preprocessing__\n",
    "In this section, a preprocessing and modeling pipeline is implemented to predict the Attrition variable in an employee dataset. Cross-validation will be used for internal evaluation (inner evaluation) and a final evaluation with an independent test set (outer evaluation).\n",
    "\n",
    "The main steps followed are:\n",
    "\n",
    "1. Splitting data into training and test sets (2/3 for training the model - 1/3 for evaluating final performance).\n",
    "2. Data preprocessing, including imputation, scaling, encoding, and dimensionality reduction.\n",
    "3. Internal evaluation (inner evaluation) using stratified cross-validation.\n",
    "4. Training and final evaluation (outer evaluation) with key metrics such as balanced accuracy, accuracy, TPR, TNR, and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1. Data Splitting into Train and Test__\n",
    "\n",
    "The predictor variables (X) are separated from the target variable (y) and the data is split into training and test sets.Se separan las variables predictoras (X) de la variable objetivo (y) y realiza la división de los datos en conjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Attrition\"])\n",
    "y = df[\"Attrition\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=1/3, random_state=100474964)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2. Data Preprocessing__\n",
    "The preprocessing is divided into three types of variables:\n",
    "\n",
    "- Numerical variables: Imputed with KNNImputer and scaled with RobustScaler.\n",
    "- Categorical variables: Imputed with the mode (most_frequent) and encoded with OneHotEncoder, followed by dimensionality reduction with PCA(n_components=5).\n",
    "- Ordinal variables: Imputed with the median and transformed with OrdinalEncoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Identification of Variable Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = X.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "ordinal_columns = [\"Education\", \"JobLevel\", \"EnvironmentSatisfaction\", \"JobSatisfaction\", \"WorkLifeBalance\", \"PerformanceRating\", \"StockOptionLevel\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Comparison Between Means and Medians\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns with significant differences between mean and median\n",
    "selected_columns = [\n",
    "    \"MonthlyIncome\", \"YearsAtCompany\", \"YearsSinceLastPromotion\",\n",
    "    \"TotalWorkingYears\", \"DistanceFromHome\", \"NumCompaniesWorked\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(selected_columns):\n",
    "    sns.histplot(df[col], kde=True, bins=30, color='blue', ax=axes[i]) \n",
    "    axes[i].axvline(df[col].mean(), color='red', linestyle='--', label='Mean')\n",
    "    axes[i].axvline(df[col].median(), color='green', linestyle='--', label='Median')\n",
    "    axes[i].set_title(f\"Distribution of {col}\")\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "1. **Significant Differences Between Mean and Median**: Variables such as _MonthlyIncome_, _YearsAtCompany_, and _YearsSinceLastPromotion_ show notable differences between their means and medians, indicating skewed distributions or outliers.  \n",
    "2. **Skewed Distributions**: Positive skew in variables like _MonthlyIncome_ and _YearsAtCompany_ suggests that means are influenced by extreme values, while medians are more robust.  \n",
    "3. **Robustness of the Median**: In skewed distributions, the median is less sensitive to outliers, making it more suitable for describing the center of the data and imputing missing values.  \n",
    "\n",
    "Given that many variables show significant differences between mean and median, along with skewed distributions, it is concluded that the __median__ is a more robust and representative strategy for imputing missing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Transformations by Variable Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Label Encoding to ordinal variables\n",
    "ord_encoder = OrdinalEncoder()\n",
    "df[ordinal_columns] = ord_encoder.fit_transform(df[ordinal_columns])\n",
    "\n",
    "ord_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "# Pipeline for categorical data\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "    ('pca', PCA(n_components=5))\n",
    "])\n",
    "\n",
    "# Pipeline for numerical data (imputation with median)\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# ColumnTransformer with all preprocessing steps\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_transformer, numerical_columns),\n",
    "    ('cat', cat_transformer, categorical_columns),\n",
    "    ('ord', ord_transformer, ordinal_columns)\n",
    "])\n",
    "\n",
    "preprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __2.4 Evaluation of Models with SMOTE__\n",
    "\n",
    "In this section, tests are conducted by applying **SMOTE (Synthetic Minority Over-sampling Technique)** to evaluate its impact on the performance of classification models.\n",
    "\n",
    "SMOTE allows balancing the class distribution, reducing the model's bias toward the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# Apply the preprocessing pipeline to X_train\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Apply SMOTE to the transformed data\n",
    "smote = SMOTE(random_state=100474964)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_transformed, y_train)\n",
    "\n",
    "print(f\"Distribution after applying SMOTE:\\n{pd.Series(y_train_resampled).value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras aplicar SMOTE, se ha podido ver que los resultados obtenidos muestran una ligera mejora en las métricas de rendimiento, especialmente en el True Positive Rate (TPR), lo que indica que los modelos son más efectivos en la detección de la clase minoritaria. \n",
    "\n",
    "Además, el coste computacional de aplicar SMOTE ha sido bajo, lo que lo convierte en una técnica viable para mejorar el equilibrio de clases sin afectar significativamente la eficiencia del entrenamiento y la inferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __3. Conversion of Target Variable (Attrition)__\n",
    "Convert the Attrition variable to a binary format (1 for \"Yes\" and 0 for \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.map({\"Yes\": 1, \"No\": 0})\n",
    "y_test = y_test.map({\"Yes\": 1, \"No\": 0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __4. Application of the Preprocessor__\n",
    "The preprocessor is fitted and transforms the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Evaluates a classification model and displays key metrics, confusion matrix, and ROC curve.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    tpr = recall_score(y_test, y_pred)  # Sensitivity / TPR\n",
    "    tnr = recall_score(y_test, y_pred, pos_label=0)  # Specificity / TNR\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"=== Final Evaluation: {model_name} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {bal_acc:.4f}\")\n",
    "    print(f\"TPR (Sensitivity): {tpr:.4f}\")\n",
    "    print(f\"TNR (Specificity): {tnr:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(f\"Confusion Matrix ({model_name})\")\n",
    "    plt.show()\n",
    "\n",
    "    if y_prob is not None:\n",
    "        fpr, tpr_curve, _ = roc_curve(y_test, y_prob)\n",
    "        auc_score = auc(fpr, tpr_curve)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr_curve, label=f'{model_name} (AUC = {auc_score:.4f})', color='blue')\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  \n",
    "        \n",
    "        plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "        plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "        plt.title(f\"ROC Curve for {model_name}\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "        return {\"Accuracy\": acc, \"Balanced Accuracy\": bal_acc, \"TPR\": tpr, \"TNR\": tnr, \"AUC\": auc_score}\n",
    "    \n",
    "    return {\"Accuracy\": acc, \"Balanced Accuracy\": bal_acc, \"TPR\": tpr, \"TNR\": tnr, \"AUC\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __5. Evaluation with Dummy Model__\n",
    "\n",
    "A Dummy model is trained to establish reference points:\n",
    "\n",
    "- Expected Balanced Accuracy if the model were trivial.\n",
    "- Comparison with trained models to demonstrate their effectiveness.\n",
    "\n",
    "This will verify if the models are truly learning patterns or simply reflecting class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train_transformed, y_train)\n",
    "dummy_ev = evaluate_model(dummy, X_test_transformed, y_test, \"Dummy Model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __6. Definition of Models with Pipeline__\n",
    "A pipeline is built that includes preprocessing and the classification model (_DecisionTreeClassifier_ and _KNeighborsClassifier_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', tree.DecisionTreeClassifier(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "clf_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knn = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "clf_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __7. Internal Evaluation (Inner Evaluation)__\n",
    "For internal evaluation, stratified cross-validation (StratifiedKFold) with 5 splits (n_splits=5) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=100474964)\n",
    "cross_val_scores = cross_val_score(clf_tree, X_train, y_train, cv=skf, scoring='balanced_accuracy')\n",
    "print(f\"Balanced Accuracy (inner evaluation): {np.mean(cross_val_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __8. Model Train__\n",
    "The model is fitted with the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "clf_tree.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "tree_time = end_time - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "clf_knn.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "knn_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __9. Evaluation with Test (Outer Evaluation)__\n",
    "Predictions are made on the test set and the main metrics are calculated:\n",
    "\n",
    "- Balanced Accuracy: Average of TPR and TNR.\n",
    "- Accuracy: Proportion of correct predictions.\n",
    "- TPR (Sensitivity/Recall): How well the model detects positive cases.\n",
    "- TNR (Specificity): How well the model detects negative cases.\n",
    "- Confusion Matrix: Detailed visualization of hits and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tree_ev = evaluate_model(clf_tree, X_test, y_test, \"Decision Tree (Base)\")\n",
    "knn_ev = evaluate_model(clf_knn, X_test, y_test, \"KNN (Base)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __10. Model Optimization with RandomizedSearchCV__\n",
    "\n",
    "Randomized hyperparameter search (_RandomizedSearchCV_) is used to find the best configuration for the models to improve their performance by efficiently tuning their hyperparameters.\n",
    "\n",
    "- KNN (_n_neighbors_, _weights_, _metric_).  \n",
    "- Decision Tree (_max_depth_, _criterion_).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist_knn = {\"n_neighbors\": randint(1, 20), \"weights\": [\"uniform\", \"distance\"], \"metric\": [\"euclidean\", \"manhattan\"]}\n",
    "param_dist_tree = {\"max_depth\": randint(3, 20), \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "random_knn = RandomizedSearchCV(KNeighborsClassifier(), \n",
    "                                param_distributions=param_dist_knn, \n",
    "                                n_iter=10, cv=5, \n",
    "                                scoring=\"balanced_accuracy\", \n",
    "                                n_jobs=-1, random_state=100474964)\n",
    "random_tree = RandomizedSearchCV(DecisionTreeClassifier(), \n",
    "                                 param_distributions=param_dist_tree, \n",
    "                                 n_iter=10, cv=5, \n",
    "                                 scoring=\"balanced_accuracy\", \n",
    "                                 n_jobs=-1, random_state=100474964)\n",
    "\n",
    "start_time = time.time()\n",
    "random_knn.fit(X_train_transformed, y_train)\n",
    "end_time = time.time()\n",
    "knn_hpo_time = end_time - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "random_tree.fit(X_train_transformed, y_train)\n",
    "end_time = time.time()\n",
    "tree_hpo_time = end_time - start_time\n",
    "\n",
    "print(f\"Best hyperparameters for KNN: {random_knn.best_params_}\")\n",
    "print(f\"Balanced Accuracy KNN: {random_knn.best_score_:.4f}\")\n",
    "print(f\"Best hyperparameters for Decision Tree: {random_tree.best_params_}\")\n",
    "print(f\"Balanced Accuracy Decision Tree: {random_tree.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tests of Distances and Their Impact on the Model\n",
    "Various values for distance metrics (_euclidean_ and _manhattan_) have been tested in the KNN model, along with different hyperparameter configurations (_n_neighbors_ and _weights_), to find the optimal values. \n",
    "\n",
    "Now, the impact on the performance of the optimized model will be evaluated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results from RandomizedSearchCV for Decision Tree\n",
    "tree_results = pd.DataFrame(random_tree.cv_results_)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(x=tree_results[\"param_max_depth\"], y=tree_results[\"mean_test_score\"], marker=\"o\", label=\"Balanced Accuracy\")\n",
    "plt.xlabel(\"Max Depth (Decision Tree)\")\n",
    "plt.ylabel(\"Balanced Accuracy\")\n",
    "plt.title(\"Impact of max_depth on Balanced Accuracy (Decision Tree)\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Extract results from RandomizedSearchCV for KNN\n",
    "knn_results = pd.DataFrame(random_knn.cv_results_)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(x=knn_results[\"param_n_neighbors\"], y=knn_results[\"mean_test_score\"], marker=\"o\", label=\"Balanced Accuracy\")\n",
    "plt.xlabel(\"Number of Neighbors (KNN)\")\n",
    "plt.ylabel(\"Balanced Accuracy\")\n",
    "plt.title(\"Impact of n_neighbors on Balanced Accuracy (KNN)\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __11. Evaluation of Optimized Models__\n",
    "\n",
    "To confirm that hyperparameter optimization actually improves performance:\n",
    "- The best hyperparameters are selected and the final models are trained.\n",
    "- All metrics are recalculated to see the improvement over the default models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_tree_ev = evaluate_model(random_tree.best_estimator_, X_test_transformed, y_test, \"Decision Tree (Optimized)\")\n",
    "opt_knn_ev = evaluate_model(random_knn.best_estimator_, X_test_transformed, y_test, \"KNN (Optimized)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.1 Evaluate base vs HPO times and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative table for evaluation metrics\n",
    "results = pd.DataFrame([dummy_ev, d_tree_ev, knn_ev, opt_tree_ev, opt_knn_ev],\n",
    "                       index=[\"Dummy\", \"Dec. Tree (Base)\", \"KNN (Base)\", \"Dec. Tree (Opt)\", \"KNN (Opt)\"])\n",
    "print(results, '\\n')\n",
    "\n",
    "\n",
    "# Comparative table for training times\n",
    "time_comparison = pd.DataFrame({\n",
    "    \"Base\": [tree_time, knn_time],\n",
    "    \"HPO\": [tree_hpo_time, knn_hpo_time]\n",
    "}, index=[\"Decision Tree\", \"KNN\"])\n",
    "\n",
    "print(time_comparison)\n",
    "\n",
    "# Comparative bar plot\n",
    "time_comparison.plot(kind=\"bar\", figsize=(5, 3), legend=True)\n",
    "plt.ylabel(\"Training Time (s)\", fontsize=10)\n",
    "plt.title(\"Comparison of Training Time Between Base and Optimized Models\", fontsize=12)\n",
    "plt.xticks(rotation=0, fontsize=9)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Conclusions of the Evaluation Comparison__\n",
    "\n",
    "__Impact of Models on Accuracy and Balance__\n",
    "\n",
    "(These results have been obtained from a specific evaluation and may vary slightly.)\n",
    "\n",
    "- The **Dummy Model** has a **Balanced Accuracy of 0.50**, indicating that it is not learning anything and simply predicts the majority class.\n",
    "- The **Decision Tree (Base)** significantly outperforms the Dummy model, with a **Balanced Accuracy of 0.7998** and a **TPR of 0.6519**, showing that it is better at detecting the minority class.\n",
    "- The **KNN (Base)** performs worse than the decision tree, with a **Balanced Accuracy of 0.6187** and a **TPR of 0.2848**, indicating difficulties in detecting the minority class.\n",
    "\n",
    "__Impact of Hyperparameter Optimization (HPO)__\n",
    "- **Optimized Decision Tree** improves over the base model in **all metrics**, achieving a **Balanced Accuracy of 0.8456** (+4.6%) and a **TPR of 0.7215** (+6.9%), indicating better detection of the minority class without losing precision in the majority class.\n",
    "- **Optimized KNN** improves significantly, with a **Balanced Accuracy of 0.8408** (+22.2%) and a **TPR of 0.6962** (+41.1%), showing that optimization has made KNN much more effective at detecting the minority class.\n",
    "\n",
    "__Impact on Training Time__\n",
    "- The **Base Decision Tree** trains in **0.1218 s**, while its optimized version takes **0.5518 s** (**4.5 times slower**), but with a significant improvement in performance.\n",
    "- The **Base KNN** trains in **0.0472 s**, while the optimized version increases to **0.4834 s** (**10.2 times slower**), although the improvement in TPR and Balanced Accuracy justifies this increase in time.\n",
    "\n",
    "##### __Final Conclusion__\n",
    "1. Decision Tree is the best overall model, with a Balanced Accuracy of 0.8456 after optimization and good detection of the minority class.\n",
    "2. KNN improves significantly with HPO, achieving a competitive TPR, although it remains more computationally expensive.\n",
    "3. Training time for optimized models increases, but it remains manageable for Decision Tree, while for KNN it is considerably higher.\n",
    "4. HPO optimization is effective and justified, as it improves minority class detection and overall accuracy without an excessive computational cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Use of AI in the Development of the Work__\n",
    "In the development of this work, artificial intelligence (ChatGPT) has been used as a support tool to facilitate the understanding of concepts, receive methodological and code suggestions, and improve the documentation writing.\n",
    "\n",
    "#### __1. Generation of Ideas and Recommendations__\n",
    "\n",
    "For project planning and model implementation, AI has been used to obtain methodological suggestions and alternative approaches in the following aspects:\n",
    "\n",
    "- Selection of preprocessing methods: Different strategies for data imputation, variable scaling, and categorical data encoding have been explored.\n",
    "- Model comparison: Recommendations have been received on how to structure the evaluation of base models versus optimized models through hyperparameter tuning.\n",
    "- Code improvement: It has helped improve and refactor some code blocks to enhance their efficiency and readability.\n",
    "\n",
    "#### __2. Explanation of Concepts__\n",
    "\n",
    "At certain points during the development, AI has been consulted for detailed explanations on technical aspects such as:\n",
    "\n",
    "- Meaning and interpretation of metrics like balanced accuracy, TPR, and TNR.\n",
    "- Differences between internal evaluation (inner evaluation) and external evaluation (outer evaluation).\n",
    "- Hyperparameter optimization using RandomizedSearchCV and its application in model selection.\n",
    "\n",
    "#### __3. Support in Documentation Writing__\n",
    "For the preparation of the report, AI has served as an assistant in:\n",
    "\n",
    "- Organizing sections and document structure.\n",
    "- Writing clearer and more concise explanations.\n",
    "- Formulating conclusions based on the obtained results.\n",
    "- Style correction and clarity in the exposition of ideas.\n",
    "- Writing the section 'Use of AI in the Development of the Work'\n",
    "\n",
    "The use of AI in this work has been strictly as support in the learning process, idea generation, and documentation writing, always supervising and verifying the results without replacing the execution of analyses, code implementation, or result interpretation. All final decisions have been made autonomously, based on the analysis and understanding of the problem.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
