{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carlos Bravo Garrán - 100474964"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # __Exploratory Data Analysis (EDA)__ \n",
    "\n",
    " In this notebook, a simplified Exploratory Data Analysis (EDA) will be performed on the provided dataset, with the objective of analyzing and understanding the factors that influence employee attrition in an organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. __Load Libraries and Data__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis and wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, recall_score, roc_curve, auc, confusion_matrix, classification_report)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import randint\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Warnings configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create virtual environment and install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install venv and requirements\n",
    "# !python3 -m venv venv\n",
    "# !source venv/bin/activate\n",
    "# %pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from the CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/attrition_availabledata_03.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. __Initial Exploration__\n",
    "\n",
    "Review the general structure of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shape = df.shape\n",
    "print(f\"The dataset contains {dataset_shape[0]} rows and {dataset_shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Attrition']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a __classification__ problem, as the target variable (Attrition) is binary (Yes / No). This means that the model must predict whether an employee will leave the company or not, rather than predicting a numerical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. __Identify Categorical and Numerical Variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "print(\"Categorical variables:\", categorical_columns)\n",
    "print(\"Numerical variables:\", numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Reclassify variables by adding ordinals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "ordinal_columns = [\"Education\", \"JobLevel\", \"EnvironmentSatisfaction\", \"JobSatisfaction\", \"WorkLifeBalance\", \"PerformanceRating\", \"StockOptionLevel\"]\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Remove from numerical the ones we have classified as ordinal\n",
    "numerical_columns = [col for col in numerical_columns if col not in ordinal_columns]\n",
    "\n",
    "print(\"Categorical variables:\", categorical_columns)\n",
    "print(\"Ordinal variables:\", ordinal_columns)\n",
    "print(\"Numerical variables:\", numerical_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Detect high cardinality categorical variables\n",
    "\n",
    "Identify categorical variables that may generate too many columns when encoding them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cardinality = df[categorical_columns].nunique().sort_values(ascending=False)\n",
    "display(categorical_cardinality)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not considered that there are high cardinality categorical variables, therefore no additional grouping or different encoding will be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. __Analysis of the Target Variable__\n",
    "\n",
    "Review the distribution of the target variable to identify class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Attrition\" in df.columns:\n",
    "    plt.figure(figsize=(4,4))\n",
    "    sns.countplot(x=df[\"Attrition\"], palette=\"viridis\")\n",
    "    plt.title(\"Distribution of the Target Variable (Attrition)\\n\")\n",
    "    plt.show()\n",
    "    \n",
    "    attrition_counts = df[\"Attrition\"].value_counts(normalize=True)\n",
    "    display(pd.DataFrame(attrition_counts).rename(columns={\"Attrition\": \"Proportion\"}).reset_index(drop=True)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Attrition.value_counts().sort_index().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is imbalanced, with 2466 employees not leaving the company (NO) and 474 who do leave (YES).\n",
    "\n",
    "This means that the majority of employees do not leave the company, which could cause a poorly trained model to always predict \"No\", achieving an apparently high accuracy, but without actually capturing the cases of attrition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. __Identify Missing Values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0].to_frame().reset_index()\n",
    "missing_values.columns = [\"Column Name\", \"Missing Values\"]\n",
    "\n",
    "display(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the missing values are very few compared to the total number of data, we will take the following measures:\n",
    "- Impute missing values in numerical variables with the median\n",
    "- Impute missing values in categorical variables with the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns differentiated by type\n",
    "num_cols = [col for col in df.select_dtypes(include=['number']).columns if col not in ordinal_columns]\n",
    "ord_cols = ordinal_columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. __Identify Constant and Identifying Variables__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Check for columns with unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df.nunique()\n",
    "\n",
    "constant_columns = df.nunique()[df.nunique() == 1].to_frame().reset_index()\n",
    "constant_columns.columns = [\"Column Name\", \"Unique Value Count\"]\n",
    "constant_columns[\"Unique Value\"] = constant_columns[\"Column Name\"].apply(lambda col: df[col].unique()[0])\n",
    "constant_columns_list = constant_columns[\"Column Name\"].tolist()\n",
    "\n",
    "display(constant_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Check for columns with ID variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of rows: {len(df)}\")\n",
    "\n",
    "unique_values = df.nunique()\n",
    "print(\"Number of unique values per column:\")\n",
    "print(unique_values, \"\\n\")\n",
    "\n",
    "id_columns = [col for col in df.columns if df[col].nunique() == len(df)]\n",
    "print(\"Identifying columns detected:\", id_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As its name indicates, _EmployeeID_ is an identifying variable, making it redundant for the study.\n",
    "\n",
    "On the other hand, the _hours_ column has 2939 values, 1 less than the total number of rows. It might be considered identifying, but upon checking the values, we see that they are simply different decimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Remove Constant and Identifying Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=constant_columns_list + id_columns, errors='ignore')\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "print('The following columns have been removed:', constant_columns_list + id_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. __Create Correlation Matrix__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the correlation matrix to understand relationships between numerical variables, having already removed constant and identifying variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df.select_dtypes(include=['number'])\n",
    "\n",
    "correlation_matrix = df_numeric.corr()\n",
    "\n",
    "plt.figure(figsize=(18, 18))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", linewidths=0.5, cmap=\"coolwarm\")\n",
    "plt.xticks(rotation=75)\n",
    "plt.title(\"Correlation Matrix\\n\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no extremely high correlations, close to 1 or -1, so the variables are not redundant nor strongly dependent on each other. However, there are some moderate correlations that we can consider.\n",
    "\n",
    "__Relationship between tenure and work experience:__\n",
    "\n",
    "- _YearsAtCompany_ and _YearsWithCurrManager_ (0.76): Employees who have been with the company longer are more likely to have been with the same manager for a longer time.\n",
    "- _YearsAtCompany_ and _TotalWorkingYears_ (0.62): The longer a person has worked in general, the more time they may have spent at the current company.\n",
    "\n",
    "__Relationship between _PercentSalaryHike_ and _PerformanceRating_ (0.78):__ There is a high correlation between salary increase and performance rating. Employees with better performance receive higher salary increases.\n",
    "\n",
    "- It could be evaluated if _PercentSalaryHike_ is redundant, as it is strongly linked to _PerformanceRating_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the rest, they have very low correlations with all the others, indicating that they may be independent or influenced by other factors not considered. Thus, it could be reviewed if these variables have any significant impact on the target variable, or if they can be eliminated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. __Identify Correlation with the Target Variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target variable to numeric\n",
    "df_aux = df.copy()\n",
    "df_aux[\"Attrition\"] = df_aux[\"Attrition\"].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "# Ensure we only work with numeric columns\n",
    "df_corr = df_aux.select_dtypes(include=['number'])\n",
    "attrition_correlation = df_corr.corr()[\"Attrition\"].sort_values()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(y=attrition_correlation.index, x=attrition_correlation.values, palette=\"coolwarm\")\n",
    "\n",
    "# Add values on the bars\n",
    "for index, value in enumerate(attrition_correlation.values):\n",
    "    ax.text(value, index, f\"{value:.2f}\", ha=\"left\", va=\"center\", fontsize=10, color=\"black\")\n",
    "\n",
    "plt.title(\"Correlation of 'Attrition' with other variables\")\n",
    "plt.xlabel(\"Correlation\")\n",
    "plt.ylabel(\"Variables\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede comprobar que no hay una variable con una correlación extremadamente fuerte con Attrition, pero sí que muchas de ellas tienen una relación muy baja, por ello cabría la posibilidad de considerar su elimiación con el fin de simplificar el modelo.\n",
    "\n",
    "Junto con las horas trabajadas, las variables de antigüedad (_YearsAtCompany_, _TotalWorkingYears_) son las que más influyen en la retención, se pueden evaluar las relaciones con _YearsWithCurrManager_ para comprobar si esta última sería redundante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se podrían eliminar las variables que tienen muy baja correlación con la objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter variables with correlation less than 0.05 in absolute value\n",
    "low_corr_columns = attrition_correlation[abs(attrition_correlation) < 0.05].index.tolist()\n",
    "\n",
    "# Remove these columns from the dataset\n",
    "df_filtered = df.drop(columns=low_corr_columns, errors='ignore')\n",
    "\n",
    "print(f\"The following columns have been removed in a test dataframe due to low correlation: {low_corr_columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __9. Visualize Relationships Between Correlated Variables__\n",
    "Perform a visual exploration of the relationships between highly correlated variables to help decide if there are redundancies or if some variables need to be transformed before using them in a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationships between tenure variables with pairplot\n",
    "sns.pairplot(df, vars=[\"YearsAtCompany\", \"YearsWithCurrManager\", \"TotalWorkingYears\"], diag_kind=\"kde\")\n",
    "plt.suptitle(\"Relationships between Tenure and Work Experience\", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relationship between Tenure and Work Experience__\n",
    "\n",
    "- _YearsAtCompany_ and _YearsWithCurrManager_ (0.76): Strong relationship, possible redundancy.\n",
    "- _YearsAtCompany_ and _TotalWorkingYears_ (0.62): Expected relationship, but provides different information.\n",
    "- Conclusion: _YearsAtCompany_ or _YearsWithCurrManager_ could be redundant.\n",
    "\n",
    "It could be evaluated for impact on the model and one could be removed if necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having identified and evaluated the variables, checking for null values, constant and identifying values, and correlations between them, we can proceed to the evaluation of potential classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Evaluation of Classification Models with Advanced Preprocessing__\n",
    "In this section, a preprocessing and modeling pipeline is implemented to predict the Attrition variable in an employee dataset. Cross-validation will be used for internal evaluation (inner evaluation) and a final evaluation with an independent test set (outer evaluation).\n",
    "\n",
    "The main steps followed are:\n",
    "\n",
    "1. Splitting data into training and test sets (2/3 for training the model - 1/3 for evaluating final performance).\n",
    "2. Data preprocessing, including imputation, scaling, encoding, and dimensionality reduction.\n",
    "3. Internal evaluation (inner evaluation) using stratified cross-validation.\n",
    "4. Training and final evaluation (outer evaluation) with key metrics such as balanced accuracy, accuracy, TPR, TNR, and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1. Data Splitting into Train and Test__\n",
    "\n",
    "The predictor variables (X) are separated from the target variable (y) and the data is split into training and test sets.Se separan las variables predictoras (X) de la variable objetivo (y) y realiza la división de los datos en conjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Attrition\"])\n",
    "y = df[\"Attrition\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=1/3, random_state=100474964)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2. Data Preprocessing__\n",
    "\n",
    "The preprocessing steps for each variable type are as follows:\n",
    "\n",
    "1. **_Ordinal Variables_**:  \n",
    "    - Applied Label Encoding using _OrdinalEncoder_.\n",
    "    - Missing values are imputed with the median using _SimpleImputer_.\n",
    "\n",
    "2. **_Categorical Variables_**:  \n",
    "    - Missing values are imputed with the most frequent value (_most_frequent_) using _SimpleImputer_.\n",
    "    - Encoded with _OneHotEncoder_ to handle categorical data.\n",
    "    - Dimensionality reduction is applied using _PCA_ with _n_components=5_.\n",
    "\n",
    "3. **_Numerical Variables_**:  \n",
    "    - Missing values are imputed with the median using _SimpleImputer_.\n",
    "    - Scaled using _RobustScaler_ to handle outliers effectively.\n",
    "\n",
    "A _ColumnTransformer_ is used to combine these preprocessing steps into a single pipeline for efficient application to the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Identification of Variable Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = X.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "ordinal_columns = [\"Education\", \"JobLevel\", \"EnvironmentSatisfaction\", \"JobSatisfaction\", \"WorkLifeBalance\", \"PerformanceRating\", \"StockOptionLevel\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Comparison Between Means and Medians\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns with significant differences between mean and median\n",
    "selected_columns = [\n",
    "    \"MonthlyIncome\", \"YearsAtCompany\", \"YearsSinceLastPromotion\",\n",
    "    \"TotalWorkingYears\", \"DistanceFromHome\", \"NumCompaniesWorked\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(10, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(selected_columns):\n",
    "    sns.histplot(df[col], kde=True, bins=30, color='blue', ax=axes[i]) \n",
    "    axes[i].axvline(df[col].mean(), color='red', linestyle='--', label='Mean')\n",
    "    axes[i].axvline(df[col].median(), color='green', linestyle='--', label='Median')\n",
    "    axes[i].set_title(f\"Distribution of {col}\")\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "1. **Significant Differences Between Mean and Median**: Variables such as _MonthlyIncome_, _YearsAtCompany_, and _YearsSinceLastPromotion_ show notable differences between their means and medians, indicating skewed distributions or outliers.  \n",
    "2. **Skewed Distributions**: Positive skew in variables like _MonthlyIncome_ and _YearsAtCompany_ suggests that means are influenced by extreme values, while medians are more robust.  \n",
    "3. **Robustness of the Median**: In skewed distributions, the median is less sensitive to outliers, making it more suitable for describing the center of the data and imputing missing values.  \n",
    "\n",
    "Given that many variables show significant differences between mean and median, along with skewed distributions, it is concluded that the __median__ is a more robust and representative strategy for imputing missing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 Justification for SimpleImputer\n",
    "\n",
    "During the process of imputing missing values, the use of **KNNImputer** was tested as an alternative to **SimpleImputer**. However, the results obtained with **KNNImputer** were inferior in terms of model performance. This could be due to the fact that, with very few missing values in the dataset, the use of KNNImputer introduces unnecessary noise by attempting to estimate values based on the nearest neighbors.\n",
    "\n",
    "For this reason, **SimpleImputer** was chosen, using the median imputation strategy for numerical variables and the mode for categorical variables, as this technique is more robust and suitable for datasets with few missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Transformations by Variable Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Apply Label Encoding to ordinal variables\n",
    "df[ordinal_columns] = OrdinalEncoder().fit_transform(df[ordinal_columns])\n",
    "\n",
    "# Ordinal pipeline\n",
    "ord_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "    ('pca', PCA(n_components=5))\n",
    "])\n",
    "\n",
    "# Numerical pipeline\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# Full preprocessor\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_transformer, numerical_columns),\n",
    "    ('cat', cat_transformer, categorical_columns),\n",
    "    ('ord', ord_transformer, ordinal_columns)\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Model Evaluation with SMOTE\n",
    "\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique)** has been applied directly in the model definition within the pipeline. This ensures that class balancing is integrated during the model training process, adequately representing the minority classes.\n",
    "\n",
    "After applying SMOTE, it has been verified that:\n",
    "- **Classes have been correctly balanced**, reducing bias towards the majority class.\n",
    "- **Performance metrics have improved**, especially in the True Positive Rate (TPR), indicating that the models are more effective in detecting the minority class.\n",
    "\n",
    "Additionally, the computational cost of applying SMOTE has been low, making it a viable technique to improve class balance without significantly affecting training and inference efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __3. Conversion of Target Variable (Attrition)__\n",
    "Convert the Attrition variable to a binary format (1 for \"Yes\" and 0 for \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.map({\"Yes\": 1, \"No\": 0})\n",
    "y_test = y_test.map({\"Yes\": 1, \"No\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __4. Application of the Preprocessor__\n",
    "The preprocessor is fitted and transforms the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Evaluates a classification model and displays key metrics, confusion matrix, and ROC curve.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    tpr = recall_score(y_test, y_pred)  # Sensitivity / TPR\n",
    "    tnr = recall_score(y_test, y_pred, pos_label=0)  # Specificity / TNR\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"=== Final Evaluation: {model_name} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {bal_acc:.4f}\")\n",
    "    print(f\"TPR (Sensitivity): {tpr:.4f}\")\n",
    "    print(f\"TNR (Specificity): {tnr:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=axes[0])\n",
    "    axes[0].set_xlabel(\"Predicted Labels\")\n",
    "    axes[0].set_ylabel(\"True Labels\")\n",
    "    axes[0].set_title(f\"Confusion Matrix ({model_name})\")\n",
    "\n",
    "    # ROC Curve\n",
    "    if y_prob is not None:\n",
    "        fpr, tpr_curve, _ = roc_curve(y_test, y_prob)\n",
    "        auc_score = auc(fpr, tpr_curve)\n",
    "\n",
    "        axes[1].plot(fpr, tpr_curve, label=f'{model_name} (AUC = {auc_score:.4f})', color='blue')\n",
    "        axes[1].plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        axes[1].set_xlabel(\"False Positive Rate (FPR)\")\n",
    "        axes[1].set_ylabel(\"True Positive Rate (TPR)\")\n",
    "        axes[1].set_title(f\"ROC Curve for {model_name}\")\n",
    "        axes[1].legend()\n",
    "        axes[1].grid()\n",
    "\n",
    "        return {\"Accuracy\": acc, \"Balanced Accuracy\": bal_acc, \"TPR\": tpr, \"TNR\": tnr, \"AUC\": auc_score}\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return {\"Accuracy\": acc, \"Balanced Accuracy\": bal_acc, \"TPR\": tpr, \"TNR\": tnr, \"AUC\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __5. Evaluation with Dummy Model__\n",
    "\n",
    "A Dummy model is trained to establish reference points:\n",
    "\n",
    "- Expected Balanced Accuracy if the model were trivial.\n",
    "- Comparison with trained models to demonstrate their effectiveness.\n",
    "\n",
    "This will verify if the models are truly learning patterns or simply reflecting class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train_transformed, y_train)\n",
    "dummy_ev = evaluate_model(dummy, X_test_transformed, y_test, \"Dummy Model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __6. Definition of Models with Pipeline__\n",
    "A pipeline is built that includes preprocessing and the classification model (_DecisionTreeClassifier_ and _KNeighborsClassifier_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=100474964)),\n",
    "    ('classifier', tree.DecisionTreeClassifier(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "clf_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knn = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=100474964)),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "clf_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __7. Internal Evaluation (Inner Evaluation)__\n",
    "For internal evaluation, stratified cross-validation (StratifiedKFold) with 5 splits (n_splits=5) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=100474964)\n",
    "cross_val_scores = cross_val_score(clf_tree, X_train, y_train, cv=skf, scoring='balanced_accuracy')\n",
    "print(f\"Balanced Accuracy (inner evaluation): {np.mean(cross_val_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __8. Model Train__\n",
    "The model is fitted with the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "clf_tree.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "tree_time = end_time - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "clf_knn.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "knn_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __9. Evaluation with Test (Outer Evaluation)__\n",
    "Predictions are made on the test set and the main metrics are calculated:\n",
    "\n",
    "- Balanced Accuracy: Average of TPR and TNR.\n",
    "- Accuracy: Proportion of correct predictions.\n",
    "- TPR (Sensitivity/Recall): How well the model detects positive cases.\n",
    "- TNR (Specificity): How well the model detects negative cases.\n",
    "- Confusion Matrix: Detailed visualization of hits and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tree_ev = evaluate_model(clf_tree, X_test, y_test, \"Decision Tree (Base)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_ev = evaluate_model(clf_knn, X_test, y_test, \"KNN (Base)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __10. Model Optimization with RandomizedSearchCV__\n",
    "\n",
    "Randomized hyperparameter search (_RandomizedSearchCV_) is used to find the best configuration for the models to improve their performance by efficiently tuning their hyperparameters.\n",
    "\n",
    "- KNN (_n_neighbors_, _weights_, _metric_).  \n",
    "- Decision Tree (_max_depth_, _criterion_).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist_knn = {\n",
    "    \"n_neighbors\": randint(1, 10), \n",
    "    \"weights\": [\"uniform\", \"distance\"], \n",
    "    \"metric\": [\"euclidean\", \"manhattan\"]}\n",
    "\n",
    "param_dist_tree = {\n",
    "    \"max_depth\": randint(2, 30),\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"min_samples_split\": randint(2, 10),\n",
    "    \"min_samples_leaf\": randint(1, 3),\n",
    "    \"splitter\": [\"best\"]\n",
    "}\n",
    "\n",
    "random_knn = RandomizedSearchCV(KNeighborsClassifier(), \n",
    "                                param_distributions=param_dist_knn, \n",
    "                                n_iter=20, cv=5, \n",
    "                                scoring=\"balanced_accuracy\", \n",
    "                                n_jobs=-1, random_state=100474964)\n",
    "\n",
    "random_tree = RandomizedSearchCV(DecisionTreeClassifier(), \n",
    "                                 param_distributions=param_dist_tree, \n",
    "                                 n_iter=30, cv=skf, \n",
    "                                 scoring=\"balanced_accuracy\", \n",
    "                                 n_jobs=-1, random_state=100474964)\n",
    "\n",
    "start_time = time.time()\n",
    "random_knn.fit(X_train_transformed, y_train)\n",
    "end_time = time.time()\n",
    "knn_hpo_time = end_time - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "random_tree.fit(X_train_transformed, y_train)\n",
    "end_time = time.time()\n",
    "tree_hpo_time = end_time - start_time\n",
    "\n",
    "print(f\"Best hyperparameters for KNN: {random_knn.best_params_}\")\n",
    "print(f\"Balanced Accuracy KNN: {random_knn.best_score_:.4f}\")\n",
    "print(f\"Best hyperparameters for Decision Tree: {random_tree.best_params_}\")\n",
    "print(f\"Balanced Accuracy Decision Tree: {random_tree.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tests of Distances and Their Impact on the Model\n",
    "Various values for distance metrics (_euclidean_ and _manhattan_) have been tested in the KNN model, along with different hyperparameter configurations (_n_neighbors_ and _weights_), to find the optimal values. \n",
    "\n",
    "Now, the impact on the performance of the optimized model will be evaluated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results from RandomizedSearchCV for Decision Tree\n",
    "tree_results = pd.DataFrame(random_tree.cv_results_)\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.lineplot(x=tree_results[\"param_max_depth\"], y=tree_results[\"mean_test_score\"], marker=\"o\", label=\"Balanced Accuracy\")\n",
    "plt.xlabel(\"Max Depth (Decision Tree)\")\n",
    "plt.ylabel(\"Balanced Accuracy\")\n",
    "plt.title(\"Impact of max_depth on Balanced Accuracy (Decision Tree)\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Extract results from RandomizedSearchCV for KNN\n",
    "knn_results = pd.DataFrame(random_knn.cv_results_)\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.lineplot(x=knn_results[\"param_n_neighbors\"], y=knn_results[\"mean_test_score\"], marker=\"o\", label=\"Balanced Accuracy\")\n",
    "plt.xlabel(\"Number of Neighbors (KNN)\")\n",
    "plt.ylabel(\"Balanced Accuracy\")\n",
    "plt.title(\"Impact of n_neighbors on Balanced Accuracy (KNN)\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __11. Evaluation of Optimized Models__\n",
    "\n",
    "To confirm that hyperparameter optimization actually improves performance:\n",
    "- The best hyperparameters are selected and the final models are trained.\n",
    "- All metrics are recalculated to see the improvement over the default models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_tree_ev = evaluate_model(random_tree.best_estimator_, X_test_transformed, y_test, \"Decision Tree (Optimized)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_knn_ev = evaluate_model(random_knn.best_estimator_, X_test_transformed, y_test, \"KNN (Optimized)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.1 Evaluate base vs HPO times and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative table for evaluation metrics\n",
    "results = pd.DataFrame([dummy_ev, d_tree_ev, opt_tree_ev, knn_ev, opt_knn_ev],\n",
    "                       index=[\"Dummy\", \"Dec. Tree (Base)\", \"Dec. Tree (Opt)\", \"KNN (Base)\", \"KNN (Opt)\"])\n",
    "print(results, '\\n')\n",
    "\n",
    "\n",
    "# Comparative table for training times\n",
    "time_comparison = pd.DataFrame({\n",
    "    \"Base\": [tree_time, knn_time],\n",
    "    \"HPO\": [tree_hpo_time, knn_hpo_time]\n",
    "}, index=[\"Decision Tree\", \"KNN\"])\n",
    "\n",
    "print(time_comparison)\n",
    "\n",
    "# Comparative bar plot\n",
    "time_comparison.plot(kind=\"bar\", figsize=(5, 3), legend=True)\n",
    "plt.ylabel(\"Training Time (s)\", fontsize=10)\n",
    "plt.title(\"Comparison of Training Time Between Base and Optimized Models\", fontsize=12)\n",
    "plt.xticks(rotation=0, fontsize=9)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Conclusions of the Evaluation Comparison__\n",
    "\n",
    "__Impact of Models on Accuracy and Balance__\n",
    "\n",
    "(These results have been obtained from a specific evaluation and may vary slightly.)\n",
    "\n",
    "- The **Dummy Model** has a **Balanced Accuracy of 0.5000**, indicating that it is not learning anything and simply predicts the majority class.\n",
    "- The **Decision Tree (Base)** significantly outperforms the Dummy model, with a **Balanced Accuracy of 0.8115** and a **TPR of 0.6899**, showing that it is better at detecting the minority class.\n",
    "- The **KNN (Base)** performs worse than the Decision Tree, with a **Balanced Accuracy of 0.7644** and a **TPR of 0.8354**, indicating better detection of the minority class but lower overall balance.\n",
    "\n",
    "__Impact of Hyperparameter Optimization (HPO)__\n",
    "- **Optimized Decision Tree** improves over the base model in **all metrics**, achieving a **Balanced Accuracy of 0.8578** (+4.6%) and a **TPR of 0.7595** (+6.9%), indicating better detection of the minority class without losing precision in the majority class.\n",
    "- **Optimized KNN** improves significantly, with a **Balanced Accuracy of 0.8469** (+8.2%) and a **TPR of 0.7278** (-10.8%), showing that optimization has made KNN more effective at balancing predictions, though with a slight reduction in minority class detection.\n",
    "\n",
    "__Impact on Training Time__\n",
    "- The **Base Decision Tree** trains in **0.2466 s**, while its optimized version takes **1.4145 s** (**5.7 times slower**), but with a significant improvement in performance.\n",
    "- The **Base KNN** trains in **0.0597 s**, while the optimized version increases to **4.8981 s** (**82.0 times slower**), although the improvement in Balanced Accuracy justifies this increase in time.\n",
    "\n",
    "##### __Final Conclusion__\n",
    "1. Decision Tree remains the best overall model, with a Balanced Accuracy of 0.8578 after optimization and good detection of the minority class.\n",
    "2. KNN improves significantly with HPO, achieving a competitive Balanced Accuracy, though it remains more computationally expensive.\n",
    "3. Training time for optimized models increases, but it remains manageable for Decision Tree, while for KNN it is considerably higher.\n",
    "4. HPO optimization is effective and justified, as it improves overall accuracy and balance without an excessive computational cost for Decision Tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Advanced Models\n",
    "\n",
    "In this section, advanced models such as Random Forest, linear models, and SVM (Support Vector Machines) will be evaluated to determine their performance in predicting the target variable (Attrition). These models will be compared with previously evaluated models to identify which offers the best results.\n",
    "\n",
    "__1. Random Forest:__ The Random Forest model will be evaluated in both its base and optimized configurations, using hyperparameter search techniques to improve its performance.\n",
    "\n",
    "__2. Linear Models:__ A logistic regression model, a widely used linear model for binary classification problems, will be implemented. This model will be evaluated in terms of its ability to capture linear patterns in the data.\n",
    "\n",
    "__3. SVM (Support Vector Machines):__ An SVM model with different kernels (linear, RBF) will be evaluated to determine its classification ability in a transformed feature space. Hyperparameter optimization will be used to fine-tune the model.\n",
    "\n",
    "The results and training time of these advanced models will be compared with previously evaluated models (Dummy, Decision Tree, KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1. Random Forest Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Random Forest Model with SMOTE\n",
    "\n",
    "The Random Forest model is defined and trained using a pipeline that includes preprocessing, SMOTE for class balancing, and the Random Forest classifier. \n",
    "\n",
    "The model is evaluated using stratified cross-validation to calculate the balanced accuracy. The training time is also measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RandomForest model with SMOTE\n",
    "clf_rf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=100474964)),\n",
    "    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=100474964))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "cross_val_scores_rf = cross_val_score(clf_rf, X_train, y_train, cv=skf, scoring='balanced_accuracy')\n",
    "print(f\"Balanced Accuracy (inner evaluation - Random Forest): {np.mean(cross_val_scores_rf):.4f}\")\n",
    "\n",
    "start_time = time.time()\n",
    "clf_rf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "rf_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "rf_ev = evaluate_model(clf_rf, X_test, y_test, \"Random Forest (Base)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Hyperparameter Optimization for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter distribution for Random Forest\n",
    "param_dist_rf = {\n",
    "    \"classifier__n_estimators\": randint(70, 110),\n",
    "    \"classifier__max_depth\": randint(2, 40),\n",
    "    \"classifier__min_samples_split\": randint(2, 10),\n",
    "    \"classifier__min_samples_leaf\": randint(1, 3),\n",
    "    \"classifier__max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"classifier__criterion\": [\"gini\", \"entropy\"],\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV for Random Forest\n",
    "random_rf = RandomizedSearchCV(\n",
    "    estimator=clf_rf,\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=10,\n",
    "    cv=skf,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_jobs=-1,\n",
    "    random_state=100474964,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model and measure time\n",
    "start_time = time.time()\n",
    "random_rf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "rf_hpo_time = end_time - start_time\n",
    "\n",
    "# Display results\n",
    "print(f\"Best hyperparameters for Random Forest: {random_rf.best_params_}\")\n",
    "print(f\"Balanced Accuracy Random Forest: {random_rf.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the optimized Random Forest model\n",
    "opt_rf_ev = evaluate_model(random_rf.best_estimator_, X_test, y_test, \"Random Forest (Optimized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Comparison of Evaluation Metrics and Training Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the evaluation metrics and training times\n",
    "results_rf = pd.DataFrame([rf_ev, opt_rf_ev], index=[\"Rndm Forest (Base)\", \"Rndm Forest (Opt)\"])\n",
    "results_all = pd.concat([results, results_rf])\n",
    "print(results_all)\n",
    "\n",
    "# Comparative table for training times\n",
    "time_comparison = pd.DataFrame({\n",
    "    \"Base\": [tree_time, knn_time, rf_ev[\"AUC\"]],\n",
    "    \"HPO\": [tree_hpo_time, knn_hpo_time, rf_hpo_time]\n",
    "}, index=[\"Decision Tree\", \"KNN\", \"Random Forest\"])\n",
    "\n",
    "\n",
    "print(time_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Performance of the Random Forest Model__\n",
    "\n",
    "- Random Forest (Base):  \n",
    "    - Achieves the highest AUC among base models (0.963), with a Balanced Accuracy of 0.842.  \n",
    "    - Excels in predicting the majority class (TNR = 0.988), although its sensitivity to the minority class is moderate (TPR = 0.696).\n",
    "\n",
    "- Random Forest (Optimized):  \n",
    "    - After hyperparameter optimization, it improves its Balanced Accuracy to 0.857 (+1.5%) and maintains a high AUC of 0.961.  \n",
    "    - Increases sensitivity to the minority class (TPR = 0.728) without compromising precision in the majority class (TNR = 0.985).\n",
    "\n",
    "__2. Impact of Hyperparameter Optimization__\n",
    "\n",
    "- Optimization improves the overall performance of the Random Forest model, especially in detecting the minority class.  \n",
    "- Although training time increases significantly (from 1.56 s to 13.68 s), the performance improvement justifies this computational cost.\n",
    "\n",
    "__3. Comparison with Other Models__\n",
    "\n",
    "- Optimized Random Forest is the model with the best overall performance, achieving the highest balance between sensitivity (TPR) and specificity (TNR).  \n",
    "- While the optimized Decision Tree is more efficient in training time, it does not reach the same level of precision as the Random Forest.\n",
    "\n",
    "__4. Final Recommendation__\n",
    "\n",
    "- **Recommended Model**: Optimized Random Forest, for its performance in Balanced Accuracy and AUC.  \n",
    "- **Efficient Alternative**: Optimized Decision Tree, for scenarios that require faster training times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2. LogisticRegression Without Regularization__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_plain = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        penalty=None,            # No regularization\n",
    "        solver='lbfgs',          # Compatible with no penalty\n",
    "        max_iter=1000,\n",
    "        random_state=100474964\n",
    "    ))\n",
    "])\n",
    "\n",
    "logreg_plain.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_plain_ev = evaluate_model(logreg_plain, X_test, y_test, \"Logistic Regression (No Penalty)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __3. LogisticRegression With Regularization__\n",
    "\n",
    "Three variants of the model are implemented with regularization:\n",
    "\n",
    "- L1 (Lasso): Promotes sparsity by eliminating less relevant coefficients.\n",
    "\n",
    "- L2 (Ridge): Strongly penalizes large coefficients, improving model stability.\n",
    "\n",
    "- ElasticNet: A combination of L1 and L2, balancing sparsity and stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_l1 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        penalty='l1',\n",
    "        solver='liblinear',\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=100474964\n",
    "    ))\n",
    "])\n",
    "\n",
    "logreg_l1.fit(X_train, y_train)\n",
    "\n",
    "logreg_l1_ev = evaluate_model(logreg_l1, X_test, y_test, \"Logistic Regression (L1 + Balanced)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_l2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        penalty='l2',\n",
    "        solver='lbfgs',\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=100474964\n",
    "    ))\n",
    "])\n",
    "\n",
    "logreg_l2.fit(X_train, y_train)\n",
    "\n",
    "logreg_l2_ev = evaluate_model(logreg_l2, X_test, y_test, \"Logistic Regression (L2 + Balanced)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 ElasticNet regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_en = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        penalty='elasticnet',\n",
    "        solver='saga',\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=100474964,\n",
    "        l1_ratio=0.5,  # Mix of L1 and L2\n",
    "    ))\n",
    "])\n",
    "\n",
    "start_time = time.time()\n",
    "logreg_en.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "logreg_en_time = end_time - start_time\n",
    "\n",
    "logreg_en_ev = evaluate_model(logreg_en, X_test, y_test, \"Logistic Regression (ElasticNet + Balanced)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __4. HPO Logistic Regression Models__\n",
    "\n",
    "A hyperparameter optimization process is applied using RandomizedSearchCV on a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform, uniform\n",
    "\n",
    "param_dist_logreg = {\n",
    "    \"penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "    \"C\": loguniform(0.001, 100),\n",
    "    \"solver\": [\"liblinear\", \"saga\"],\n",
    "    \"l1_ratio\": uniform(0, 1)  # Only for elasticnet\n",
    "}\n",
    "\n",
    "base_logreg = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=100474964\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_logreg = RandomizedSearchCV(\n",
    "    estimator=base_logreg,\n",
    "    param_distributions={\n",
    "        'classifier__' + k: v for k, v in param_dist_logreg.items()\n",
    "    },\n",
    "    n_iter=30,\n",
    "    cv=5,\n",
    "    scoring='balanced_accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=100474964\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "random_logreg.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "logreg_hpo_time = end_time - start_time\n",
    "\n",
    "\n",
    "print(\"Best parameters:\", random_logreg.best_params_)\n",
    "print(f\"Best Balanced Accuracy (CV): {random_logreg.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_logreg_ev = evaluate_model(random_logreg.best_estimator_, X_test, y_test, \"Logistic Regression (Optimized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __5. Logistic Regressions Evaluation and Metric Comparison__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_logreg = pd.DataFrame(\n",
    "    [logreg_plain_ev, logreg_l1_ev, logreg_l2_ev, logreg_en_ev, opt_logreg_ev],\n",
    "    index=[\"LogReg (No Penalty)\", \"LogReg (L1)\", \"LogReg (L2)\", \"LogReg (ElasticNet)\", \"LogReg (HPO)\"]\n",
    ")\n",
    "print(results_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results_logreg_plot if not already defined\n",
    "if 'results_logreg_plot' not in globals():\n",
    "    results_logreg_plot = pd.DataFrame(columns=[\"Model\", \"Metric\", \"Score\"])\n",
    "\n",
    "# Add HPO model results to the plot data\n",
    "opt_logreg_results = pd.DataFrame({\n",
    "    \"Model\": [\"LogReg (HPO)\"] * 3,\n",
    "    \"Metric\": [\"Balanced Accuracy\", \"TPR\", \"TNR\"],\n",
    "    \"Score\": [opt_logreg_ev[\"Balanced Accuracy\"], opt_logreg_ev[\"TPR\"], opt_logreg_ev[\"TNR\"]]\n",
    "})\n",
    "\n",
    "results_logreg_plot = pd.concat([results_logreg_plot, opt_logreg_results], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_results = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        \"Model\": model,\n",
    "        \"Metric\": [\"Balanced Accuracy\", \"TPR\", \"TNR\"],\n",
    "        \"Score\": [evals[\"Balanced Accuracy\"], evals[\"TPR\"], evals[\"TNR\"]]\n",
    "    }) for model, evals in [\n",
    "        (\"LogReg (No Penalty)\", logreg_plain_ev),\n",
    "        (\"LogReg (L1)\", logreg_l1_ev),\n",
    "        (\"LogReg (L2)\", logreg_l2_ev),\n",
    "        (\"LogReg (ElasticNet)\", logreg_en_ev),\n",
    "        (\"LogReg (HPO)\", opt_logreg_ev)\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Graficar los resultados\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.barplot(data=logreg_results, x=\"Model\", y=\"Score\", hue=\"Metric\", palette=\"viridis\", width=0.6)\n",
    "\n",
    "plt.title(\"Comparison of Logistic Regression Models by Metric\", fontsize=10)\n",
    "plt.xticks(rotation=20, fontsize=7)\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Score\", fontsize=10)\n",
    "plt.xlabel(\"\")\n",
    "plt.legend(title=\"Metric\", loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of various logistic regression models, both basic and optimized, has been evaluated using different forms of regularization (no regularization, L1, L2, and ElasticNet).\n",
    "\n",
    "The results show that:\n",
    "\n",
    "- The model without regularization achieves a low balanced accuracy (~0.60) and has very limited ability to detect the minority class (TPR ~0.22), making it unsuitable for this problem.\n",
    "\n",
    "- The L1 model significantly improves positive detection (TPR ~0.69) but loses precision and has a low TNR, showing limited stability.\n",
    "\n",
    "- L2 and ElasticNet offer more balanced performance (balanced accuracy ~0.69–0.70), being more robust than L1 and more consistent in predictions.\n",
    "\n",
    "- The optimized model achieves a balanced accuracy of 0.7016, with nearly equal TPR and TNR (~0.70), without overfitting and with minimal training times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison with Other Models**\n",
    "\n",
    "- Models such as KNN and Decision Trees outperform logistic regression in both balanced accuracy and AUC, especially after hyperparameter optimization.\n",
    "\n",
    "- Random Forest, even in its base version, achieves higher balanced accuracy (~0.84–0.85) with better predictive capabilities, albeit at a higher computational cost.\n",
    "\n",
    "- Logistic regression, on the other hand, trains very quickly and is easily interpretable, making it suitable for scenarios where explainability and computational efficiency are priorities, even if it is not the most accurate model.\n",
    "\n",
    "Although logistic regression models are not the most powerful in this case, they do provide reasonable results in balanced accuracy when used with regularization and class balancing, along with low training times and good interpretability. Therefore, they can be considered a useful baseline for comparison, for projects with computational constraints, or when model decisions need to be justified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __6. Support Vector Machines (SVM) Model__\n",
    "\n",
    "SVMs are linear (or nonlinear through kernels) models that maximize the margin between classes. They can be adjusted with different kernels and penalty parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1 Training SVM (Linear Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_linear = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(\n",
    "        kernel='linear',\n",
    "        class_weight='balanced',\n",
    "        probability=True,\n",
    "        random_state=100474964\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "svm_linear.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "svm_linear_time = end_time - start_time\n",
    "\n",
    "svm_linear_ev = evaluate_model(svm_linear, X_test, y_test, \"SVM (Linear)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 Evaluation with SVM (RBF Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=100474964)),\n",
    "    ('classifier', SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=100474964))\n",
    "])\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "svm_rbf_time = end_time - start_time\n",
    "\n",
    "svm_rbf_ev = evaluate_model(svm_rbf, X_test, y_test, \"SVM (RBF)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3 Comparison of SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_svm = pd.DataFrame([\n",
    "    svm_linear_ev,\n",
    "    svm_rbf_ev\n",
    "], index=[\"SVM (Linear)\", \"SVM (RBF)\"])\n",
    "\n",
    "print(results_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __7. Hyperparameter Optimization for SVM__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 HPO for SVM (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist_svm_linear = {\n",
    "    \"classifier__estimator__C\": loguniform(0.001, 100),\n",
    "}\n",
    "\n",
    "svm_linear_base = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', CalibratedClassifierCV(\n",
    "        estimator=LinearSVC(\n",
    "            class_weight='balanced',\n",
    "            max_iter=10000,\n",
    "            random_state=100474964\n",
    "        ),\n",
    "        method='sigmoid',\n",
    "        cv=5\n",
    "    ))\n",
    "])\n",
    "\n",
    "random_svm_linear = RandomizedSearchCV(\n",
    "    estimator=svm_linear_base,\n",
    "    param_distributions=param_dist_svm_linear,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='balanced_accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=100474964\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "random_svm_linear.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "svm_linear_hpo_time = end_time - start_time\n",
    "\n",
    "print(\"Best parameters (SVM Linear):\", random_svm_linear.best_params_)\n",
    "print(f\"Best Balanced Accuracy (CV): {random_svm_linear.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_linear_opt_ev = evaluate_model(random_svm_linear.best_estimator_, X_test, y_test, \"SVM (Linear Optimized)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2 HPO for SVM (RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist_svm = {\n",
    "    \"classifier__C\": uniform(0.1, 10),     # Margin penalty\n",
    "    \"classifier__gamma\": uniform(0.001, 1) # RBF kernel parameter\n",
    "}\n",
    "\n",
    "svm_rbf_base = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(\n",
    "        kernel='rbf',\n",
    "        class_weight='balanced',\n",
    "        probability=True,\n",
    "        random_state=100474964\n",
    "    ))\n",
    "])\n",
    "\n",
    "random_svm_rbf = RandomizedSearchCV(\n",
    "    estimator=svm_rbf_base,\n",
    "    param_distributions=param_dist_svm,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='balanced_accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=100474964\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "random_svm_rbf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "svm_rbf_hpo_time = end_time - start_time\n",
    "\n",
    "print(\"Best parameters (SVM-RBF):\", random_svm_rbf.best_params_)\n",
    "print(f\"Best Balanced Accuracy (CV): {random_svm_rbf.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf_opt_ev = evaluate_model(random_svm_rbf.best_estimator_, X_test, y_test, \"SVM (RBF Optimized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __8. SVM Evaluation and Metric Comparison__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The basic linear SVM model showed modest results, with a balanced accuracy close to 0.68. While it is quick to train, it fails to effectively separate classes in a nonlinear problem like this.\n",
    "\n",
    "- The SVM with RBF kernel significantly improved positive detection capability (TPR ~0.70) while maintaining a good TNR, achieving a balanced accuracy between 0.71 and 0.74 across runs.\n",
    "\n",
    "- The optimized SVM with RBF kernel achieved the highest balanced accuracy (0.87), demonstrating excellent performance in correctly balancing class detection, both positive and negative. This result highlights its effectiveness and suitability for this problem.\n",
    "\n",
    "The SVM models with RBF kernel and optimization are an excellent choice for this problem, as they adapt well to the dataset's nonlinearity while maintaining reasonable training times.\n",
    "\n",
    "Additionally, they are less interpretable than regression or tree-based models but more robust than the linear version. They are not the best overall models but are a solid and competitive alternative if hyperparameters are well-tuned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Final Conclusions of the Model's Study__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([results_all, results_logreg, results_svm, pd.DataFrame([svm_rbf_opt_ev], index=[\"SVM (RBF Opt)\"])])\n",
    "\n",
    "# Display consolidated metrics as a table\n",
    "print(\"=== Consolidated Metrics (Table Format) ===\")\n",
    "display(all_results)\n",
    "\n",
    "time_comparison[\"Model\"] = [\"Decision Tree\", \"KNN\", \"Random Forest\"]\n",
    "time_comparison = time_comparison.set_index(\"Model\")\n",
    "\n",
    "svm_opt_time = pd.DataFrame({\n",
    "    \"Base\": [svm_rbf_time],\n",
    "    \"HPO\": [svm_rbf_hpo_time]\n",
    "}, index=[\"SVM\"])\n",
    "\n",
    "logreg_opt_time = pd.DataFrame({\n",
    "    \"Base\": [logreg_en_time],\n",
    "    \"HPO\": [logreg_hpo_time]\n",
    "}, index=[\"Logistic Regression\"])\n",
    "\n",
    "time_comparison = pd.concat([time_comparison, logreg_opt_time, svm_opt_time])\n",
    "\n",
    "print(\"\\n=== Training Times (Base and HPO) ===\")\n",
    "display(time_comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Visual Evaluation of Models__\n",
    "Before drawing final conclusions, the following section presents a series of visualizations that compare the performance of the evaluated models.\n",
    "\n",
    "These visualizations provide a clear comparative perspective of the models' performance, analyzing their balance between sensitivity and specificity, their overall discriminative ability (AUC), and computational efficiency. This perspective facilitates an informed choice of the most suitable model for addressing the attrition prediction problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot 1: Distribution of Balanced Accuracy by Model\n",
    "sns.barplot(data=all_results.reset_index(), x=\"index\", y=\"Balanced Accuracy\", palette=\"viridis\", ax=axes[0])\n",
    "axes[0].set_title(\"Balanced Accuracy by Model\", fontsize=14)\n",
    "axes[0].set_xlabel(\"Model\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Balanced Accuracy\", fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=75)\n",
    "axes[0].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Plot 2: Comparison of TPR and TNR by Model\n",
    "results_all_melted_tpr_tnr = all_results[[\"TPR\", \"TNR\"]].reset_index().melt(id_vars=\"index\", var_name=\"Metric\", value_name=\"Score\")\n",
    "sns.barplot(data=results_all_melted_tpr_tnr, x=\"index\", y=\"Score\", hue=\"Metric\", palette=\"viridis\", ax=axes[1])\n",
    "axes[1].set_title(\"Comparison of TPR and TNR by Model\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Model\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Score\", fontsize=12)\n",
    "axes[1].tick_params(axis='x', rotation=75)\n",
    "axes[1].legend(title=\"Metric\", loc=\"upper right\")\n",
    "axes[1].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Plot 3: Comparison of AUC by Model\n",
    "sns.barplot(data=all_results.reset_index(), x=\"index\", y=\"AUC\", palette=\"magma\", ax=axes[2])\n",
    "axes[2].set_title(\"AUC by Model\", fontsize=14)\n",
    "axes[2].set_xlabel(\"Model\", fontsize=12)\n",
    "axes[2].set_ylabel(\"AUC\", fontsize=12)\n",
    "axes[2].tick_params(axis='x', rotation=75)\n",
    "axes[2].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Plot 4: Comparison of Training Times (Base vs HPO)\n",
    "time_comparison.plot(kind=\"bar\", ax=axes[3], color=[\"lightblue\", \"green\"])\n",
    "axes[3].set_title(\"Training Time Comparison (Base vs HPO)\", fontsize=14)\n",
    "axes[3].set_ylabel(\"Time (seconds)\", fontsize=12)\n",
    "axes[3].set_xlabel(\"Model\", fontsize=12)\n",
    "axes[3].tick_params(axis='x', rotation=30)\n",
    "axes[3].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph 1 – Balanced Accuracy by Model**\n",
    "\n",
    "- Optimized SVM RBF is the best model in terms of Balanced Accuracy (~0.87), surpassing Random Forest and Decision Tree.\n",
    "\n",
    "- Linear models (LogReg and SVM Linear) lag significantly, suggesting that the relationships in the data are not linear.\n",
    "\n",
    "**Graph 2 – TPR and TNR**\n",
    "\n",
    "- Models like Random Forest and Optimized SVM RBF have high and balanced TPR and TNR, which is ideal.\n",
    "\n",
    "- LogReg without penalty has a very low TPR, although its TNR is excellent → it means it poorly detects positive cases (attrition).\n",
    "\n",
    "**Graph 3 – AUC by Model**\n",
    "\n",
    "- Optimized SVM RBF has the highest AUC, confirming that it not only predicts accurately but also with high confidence.\n",
    "\n",
    "- Models like LogReg (No Penalty) and Dummy have much lower AUC → worse discriminative ability.\n",
    "\n",
    "**Graph 4 – Training Time Comparison**\n",
    "\n",
    "- SVM and Random Forest with HPO are the slowest, especially SVM (~30 seconds).\n",
    "\n",
    "- KNN and Decision Tree are fast even with optimization → a good option if speed is required.\n",
    "\n",
    "**General Conclusions:**\n",
    "\n",
    "- Optimized SVM RBF is the best overall model: highest Balanced Accuracy and AUC, with balanced TPR/TNR.\n",
    "\n",
    "- Random Forest also performs very well, with faster training.\n",
    "\n",
    "- Linear models (LogReg and SVM Linear) have clearly inferior performance for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Final Conclusions: Model Comparison and Utility__\n",
    "\n",
    "Based on the visual analysis of the obtained results, practical conclusions can be drawn about the behavior of each model. This section summarizes the most relevant strengths and weaknesses of each technique, considering both predictive performance and computational cost.\n",
    "\n",
    "The goal is to identify which models excel in each key metric (Balanced Accuracy, TPR, TNR, AUC) and reflect on the situations where they might be most useful: for example, prioritizing the detection of attrition cases, minimizing false positives, or achieving a good overall balance with high efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Decision Tree Models**\n",
    "- The optimized decision tree achieves a balanced accuracy of 0.85, showing competitive performance with very low training time (≈ 1.48s with HPO).\n",
    "\n",
    "- It is a simple, interpretable, and efficient model, ideal for quick solutions or resource-constrained environments.\n",
    "\n",
    "**2. KNN (K-Nearest Neighbors)**\n",
    "- Surprisingly, KNN offers good balanced accuracy (0.8469) with optimization, but its base version has much lower overall accuracy.\n",
    "\n",
    "- Its training time with HPO was notably high (~4.78s), making it less attractive if scalability is required.\n",
    "\n",
    "**3. Random Forest**\n",
    "- The base model already performs excellently, and with optimization, it achieves a balanced accuracy of 0.8566, with an outstanding AUC of 0.96.\n",
    "\n",
    "- If time and resources are available, it is one of the best overall models in terms of precision and class balance. Although HPO training time increases to 17s, it remains reasonable given its capabilities.\n",
    "\n",
    "**4. Logistic Regression**\n",
    "- Overall, its versions (penalized and unpenalized) have significantly lower balanced accuracy (≤ 0.70), although they are fast.\n",
    "\n",
    "- The optimized model improves to 0.7016 but still falls far behind more complex models.\n",
    "\n",
    "- Useful as a baseline model or when extreme interpretability is required, but not ideal for this problem.\n",
    "\n",
    "**5. SVM (Support Vector Machines)**\n",
    "- The linear SVM does not stand out, but using an optimized RBF kernel achieves the best result in the entire study with a balanced accuracy of 0.87 and an AUC of 0.92.\n",
    "\n",
    "- It is the most powerful model in terms of performance, at the cost of slow training (over 32s with HPO).\n",
    "\n",
    "**Overall Conclusion**\n",
    "\n",
    "The optimized SVM with RBF is the best classifier for this dataset, followed by the optimized Random Forest.\n",
    "\n",
    "For environments where computational cost is important, the Decision Tree may be the best option due to its balance between speed and performance.\n",
    "\n",
    "Linear models are suitable as baselines but fall short in capturing complex nonlinear relationships, as seen in this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Train and Save the Final Model__\n",
    "\n",
    "After selecting the best-performing model based on the evaluation metrics, we now retrain it on the entire training dataset to obtain the final model.\n",
    "\n",
    "In this case, the selected model is a Support Vector Machine (SVM) with an RBF kernel and optimized hyperparameters, obtained using _RandomizedSearchCV_.\n",
    "\n",
    "Once trained, we save the model to a file (_modelo_final.pkl_) so it can be used later to generate predictions on the competition dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = random_svm_rbf.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(best_model, \"modelo_final.pkl\")\n",
    "\n",
    "print(\"The trained model has been saved as 'modelo_final.pkl'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Use of AI in the Development of the Work__\n",
    "In the development of this work, artificial intelligence (ChatGPT) has been used as a support tool to facilitate the understanding of concepts, receive methodological and code suggestions, and improve the documentation writing.\n",
    "\n",
    "#### __1. Generation of Ideas and Recommendations__\n",
    "\n",
    "For project planning and model implementation, AI has been used to obtain methodological suggestions and alternative approaches in the following aspects:\n",
    "\n",
    "- Selection of preprocessing methods: Different strategies for data imputation, variable scaling, and data encoding have been explored.\n",
    "- Model comparison: Recommendations have been received on how to structure the evaluation of base models versus optimized models through hyperparameter tuning.\n",
    "- Code improvement: It has helped improve and refactor some code blocks to enhance their efficiency and readability.\n",
    "\n",
    "However, in some cases, the original idea proved to be better than the AI's recommendation. For example, AI suggested specific hyperparameter ranges for optimization, but the original idea of using a broader range allowed for better exploration and improved model performance. Similarly, while AI recommended a particular imputation strategy, the original approach resulted in more consistent results across different models.\n",
    "\n",
    "\n",
    "#### __2. Explanation of Concepts__\n",
    "\n",
    "At certain points during the development, AI has been consulted for detailed explanations on technical aspects such as:\n",
    "\n",
    "- Meaning and interpretation of metrics like balanced accuracy, TPR, and TNR.\n",
    "- Differences between internal evaluation (inner evaluation) and external evaluation (outer evaluation).\n",
    "- Hyperparameter optimization using RandomizedSearchCV and its application in model selection.\n",
    "\n",
    "#### __3. Support in Documentation Writing__\n",
    "For the preparation of the report, AI has served as an assistant in:\n",
    "\n",
    "- Organizing sections and document structure.\n",
    "- Writing clearer and more concise explanations.\n",
    "- Formulating conclusions based on the obtained results.\n",
    "- Style correction and clarity in the exposition of ideas.\n",
    "- Writing the section 'Use of AI in the Development of the Work'\n",
    "\n",
    "The use of AI in this work has been strictly as support in the learning process, idea generation, and documentation writing, always supervising and verifying the results without replacing the execution of analyses, code implementation, or result interpretation. All final decisions have been made autonomously, based on the analysis and understanding of the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
